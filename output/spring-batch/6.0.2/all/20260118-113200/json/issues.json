[
  {
    "number": 5106,
    "title": "Intermittent OptimisticLockingFailureException when starting job using jobOperator.start() with asyncTaskExecutor",
    "state": "closed",
    "created_at": "2025-11-25T03:07:33Z",
    "updated_at": "2026-01-14T15:50:39Z",
    "author": "scottgongsg",
    "url": "https://github.com/spring-projects/spring-batch/issues/5106",
    "body": "**Bug description**\nIntermittent OptimisticLockingFailureException when starting job using jobOperator.start() with asyncTaskExecutor\n\n**Environment**\nSpring Boot 4.0.0\nSpring Batch 6.0.0\nJava 21\n\n**Steps to reproduce**\n1) Create a new Spring Boot project through the Initializr with Spring Batch and Spring Data Jpa selected.\n2) Create a configuration class and annotate it with @EnableBatchProcessing and @EnableJdbcJobRepository\n3) Implement a simple job and create jobOperator using asyncTaskExecutor \n4) Using jobOperator.start() to start the job \n5) Intermittent OptimisticLockingFailureException happend in the JdbcJobExecutionDao.updateJobExecution() \n6) Based on my debug, I found that Job instance is not inserted in the BATCH_JOB_EXECUTION table sometimes but job execution is launched in a new Thread using the asyncTaskExecutor (this is in the TaskExecutorJobLauncher class),  and unable to find the job execution record in table then OptimisticLockingFailureException is happend. \n\n**Expected behavior**\nJob should run without issue always. \n",
    "labels": [
      "type: bug",
      "in: core",
      "has: votes",
      "has: minimal-example"
    ],
    "comments": [
      {
        "author": "ahoehma",
        "created_at": "2025-12-01T13:45:21Z",
        "body": "Not exactly what I'm fighting with :-) But I will watch the feedback here as well.\n\n(I started this discussion: https://github.com/spring-projects/spring-batch/discussions/5121)"
      },
      {
        "author": "phactum-mnestler",
        "created_at": "2025-12-17T14:11:43Z",
        "body": "We're seeing the same issue as described. I created a minimal reproducer here: https://github.com/phactum-mnestler/spring-batch-reproducer\nBased on the stacktrace, it appears the issue is a race condition between the async runnable of the `TaskExecutorJobLauncher` and the enclosing `finally` clause:\n```\norg.springframework.dao.OptimisticLockingFailureException: Attempt to update job execution id=1 with wrong version (0), where current version is 1\n\tat org.springframework.batch.core.repository.dao.jdbc.JdbcJobExecutionDao.updateJobExecution(JdbcJobExecutionDao.java:302) ~[spring-batch-core-6.0.1.jar:6.0.1]\n\tat org.springframework.batch.core.repository.support.SimpleJobRepository.update(SimpleJobRepository.java:152) ~[spring-batch-core-6.0.1.jar:6.0.1]\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103) ~[na:na]\n        ----- AOP traces skipped ---\n\tat jdk.proxy3/jdk.proxy3.$Proxy85.update(Unknown Source) ~[na:na]\n\tat org.springframework.batch.core.job.AbstractJob.updateStatus(AbstractJob.java:420) ~[spring-batch-core-6.0.1.jar:6.0.1]\n\tat org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:289) ~[spring-batch-core-6.0.1.jar:6.0.1]\n\tat org.springframework.batch.core.launch.support.TaskExecutorJobLauncher$1.run(TaskExecutorJobLauncher.java:220) ~[spring-batch-core-6.0.1.jar:6.0.1]\n```\nThe `finally`-clause was not present in Spring Batch 5.x, which only updated the job execution if the `Runnable` wasn't able to be scheduled.\n\nWe're seeing this issue persist even with the newly released 6.0.1 version"
      },
      {
        "author": "licenziato",
        "created_at": "2025-12-17T15:35:10Z",
        "body": "I saw the same issue and the same root cause, as workaround setting the `ThreadPoolTaskExecutor` used by `JobOperator` as a single thread executor solved the race condition, waiting for a proper fix:\n\n```\n    @Bean\n    public JobOperatorFactoryBean jobOperator(JobRepository jobRepository) {\n        ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor();\n        taskExecutor.setCorePoolSize(1);\n        taskExecutor.setMaxPoolSize(1);\n        taskExecutor.afterPropertiesSet();\n\n        JobOperatorFactoryBean jobOperatorFactoryBean = new JobOperatorFactoryBean();\n        jobOperatorFactoryBean.setJobRepository(jobRepository);\n        jobOperatorFactoryBean.setTaskExecutor(taskExecutor);\n        return jobOperatorFactoryBean;\n    }\n\n```"
      },
      {
        "author": "kizombaDev",
        "created_at": "2025-12-19T13:19:53Z",
        "body": "We are currently unfortunately running into the same problem with Spring Batch 6.0.1, MongoDB, and a `ThreadPoolTaskExecutor`.\n\nI start a job using `jobOperator.start(job, new JobParameters())` and immediately get a `DataIntegrityViolationException`.\n\nI can confirm that the problem is caused by the call to `this.jobRepository.update(jobExecution);` in the finally block of the method\n`org.springframework.batch.core.launch.support.TaskExecutorJobLauncher#launchJobExecution`.\n\nI created a reproducer with a mongoDB: https://github.com/kizombaDev/spring-batch-async-bug-reproducer"
      },
      {
        "author": "banseok1216",
        "created_at": "2025-12-21T08:45:36Z",
        "body": "In TaskExecutorJobLauncher.launchJobExecution(..), consider removing the unconditional jobRepository.update(jobExecution) after successful submission to the TaskExecutor, and keep the update only in the TaskRejectedException path.\n\nFor accepted tasks, the job thread will update the JobExecution anyway; the extra launcher-thread update can race and avoid trigger OptimisticLockingFailureException.\n\n```java\ncatch (TaskRejectedException e) {\n    jobExecution.upgradeStatus(BatchStatus.FAILED);\n    if (ExitStatus.UNKNOWN.equals(jobExecution.getExitStatus())) {\n        jobExecution.setExitStatus(ExitStatus.FAILED.addExitDescription(e));\n    }\n    // keep this: the job thread will never run in this case\n    this.jobRepository.update(jobExecution);\n}\n\n// no unconditional update here: for accepted tasks, the job thread persists JobExecution updates\n```"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2025-12-21T13:21:29Z",
        "body": "Thank you all for reporting this issue and for providing analysis / reproducer!\n\nThis seems like a regression in #3637. I will plan the fix for the next patch version 6.0.2."
      },
      {
        "author": "StefanMuellerCH",
        "created_at": "2026-01-05T14:26:24Z",
        "body": "Same problem here, but the fix from [licenziato](https://github.com/licenziato) above did not help, as the `ThreadPoolTaskExecutor`, even with size 1, executes the job itself in another thread as the `TaskExecutorJobLauncher `calls the update. I had to switch to the `SyncTaskExecutor` for the bug to be solved:\n\n\n```\n@Bean\npublic JobOperatorFactoryBean jobOperator(JobRepository jobRepository) {\n  var taskExecutor = new SyncTaskExecutor();\n  var jobOperatorFactoryBean = new JobOperatorFactoryBean();\n  jobOperatorFactoryBean.setJobRepository(jobRepository);\n  jobOperatorFactoryBean.setTaskExecutor(taskExecutor);\n  return jobOperatorFactoryBean;\n}\n```\n\nUsing the SyncTaskExecutor has considerable drawbacks, we cannot use this for production, so we have to wait for the fix."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": [
        "b024116968ac5dd89ea84a8a3048d0e4a39d7519",
        "76e723e41939b1ab6910f9ce8d61053abb1d0575"
      ]
    }
  },
  {
    "number": 5109,
    "title": "Incorrect resource cleanup order in AbstractCursorItemReader#doClose leads to inconsistent behavior",
    "state": "open",
    "created_at": "2025-11-25T15:30:46Z",
    "updated_at": "2026-01-13T11:38:09Z",
    "author": "banseok1216",
    "url": "https://github.com/spring-projects/spring-batch/issues/5109",
    "body": "**Bug description**\nAbstractCursorItemReader#doClose closes JDBC resources in an incorrect order\n\n**Environment**\nSpring Batch: 6.0.0\njava: Java 21\n**Steps to reproduce**\nSteps to reproduce the issue.\n\n**Expected behavior**\n1. Create a simple JdbcCursorItemReader that opens a cursor.\n2. Call `reader.open(executionContext)`.\n3. Call `reader.close()`.\n4. Observe that:\n   - `cleanupOnClose(connection)` is invoked after the connection is already closed.\n   - `setAutoCommit(initialAutoCommit)` is never executed because the connection is closed.\n\nExample of the problematic execution order:\n\n```java\nJdbcUtils.closeConnection(this.con);   // connection is closed here\n\ncleanupOnClose(this.con);              // executed after close\n// con.isClosed() == true\n\nif (this.con != null && !this.con.isClosed()) {\n    this.con.setAutoCommit(initialConnectionAutoCommit);  // skipped\n}\n```\n\n**Additional note on responsibility**\n\nCurrently, `doClose()` ends up closing the `Connection` even though the\nconnection is created and owned by `AbstractCursorItemReader`. This leads to a\nmixed ownership model:\n\n- the parent opens the connection,\n- the child performs cursor-level cleanup,\n- but the child also closes the connection.\n\nIt is more consistent for the component that creates the connection to be the\none responsible for closing it. The reader subclass should only release\ncursor-related resources such as the `ResultSet` and `PreparedStatement`.\n\nThe proposed change aligns the close behavior with that ownership model.",
    "labels": [
      "in: infrastructure",
      "type: bug",
      "for: backport-to-5.2.x"
    ],
    "comments": [],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5136,
    "title": "The implementation of jumpToItem(int itemLastIndex) in AbstractPaginatedDataItemReader does not handle restart behavior correctly.",
    "state": "open",
    "created_at": "2025-12-05T15:04:20Z",
    "updated_at": "2026-01-13T11:34:25Z",
    "author": "banseok1216",
    "url": "https://github.com/spring-projects/spring-batch/issues/5136",
    "body": "**Bug description**\n\nThe `jumpToItem(int itemLastIndex)` implementation in `AbstractPaginatedDataItemReader` does not correctly restore the reader position when a step is restarted. In practice, this leads to skipped items or the reader resuming from an unexpected location.\n\nWhile the method is intended to position the reader so that the next call to `read()` returns the item at the given index, the current implementation does not behave that way.\n\n### Parent class implementation\n\n```java\nprotected void jumpToItem(int itemIndex) throws Exception {\n    for (int i = 0; i < itemIndex; i++) {\n        read();\n    }\n}\n```\n\nThis implementation ensures:\n\n- After calling `jumpToItem(n)`, invoking `read()` returns the **nth item**.\n\n---\n\n### Overridden implementation in AbstractPaginatedDataItemReader\n\n```java\n@Override\nprotected void jumpToItem(int itemLastIndex) throws Exception {\n    this.lock.lock();\n    try {\n        page = itemLastIndex / pageSize;\n        int current = itemLastIndex % pageSize;\n\n        Iterator<T> initialPage = doPageRead();\n\n        for (; current >= 0; current--) {\n            initialPage.next();\n        }\n    }\n    finally {\n        this.lock.unlock();\n    }\n}\n```\n\n\n\nThere are two main issues:\n\n---\n\n### 1. Off-by-one advancement\n\nBecause the loop condition uses `current >= 0`, the iterator advances one time too many.  \nFor example, calling `jumpToItem(7)` causes the next `read()` to return the item at index 8 instead of 7.\n\n| Call | Expected | Actual |\n|------|----------|--------|\n| `jumpToItem(7)` â†’ `read()` | 7 | 8 |\n\nThis breaks the expectation that the reader should restart exactly at the stored index.\n\n---\n\n### 2. Iterator not assigned to the reader state\n\nThe method advances an iterator but never assigns it to `results`. On the next `read()` call, a new page is loaded, undoing any positioning work done inside `jumpToItem`. This makes the restart position unreliable and can result in the first page being re-read or the reader landing on the wrong item.\n\n---\n\n**Environment**\n\n- Spring Batch: 5.x  \n- Java: 17  \n\n---\n\n**Steps to reproduce**\n\n1. Implement a paginated reader similar to the one in the test below.\n2. Call `open(new ExecutionContext())`.\n3. Invoke `jumpToItem(n)`.\n4. Call `read()`.\n5. The value returned will not match the expected item at index `n`.\n\n---\n\n**Expected behavior**\n\nAfter calling `jumpToItem(n)`, the following `read()` call should return the item located at index `n`.\n\n---\n\n**Minimal Complete Reproducible example**\n\n```java\npackage org.springframework.batch.infrastructure.item.data;\n\nimport org.junit.jupiter.api.Test;\nimport org.springframework.batch.infrastructure.item.ExecutionContext;\n\nimport java.util.Iterator;\nimport java.util.List;\n\nimport static org.junit.jupiter.api.Assertions.*;\n\nclass AbstractPaginatedDataItemReaderTests {\n\n  static class PaginatedDataItemReader extends AbstractPaginatedDataItemReader<Integer> {\n\n    private final List<Integer> data = List.of(\n        0,1,2,3,4,5,6,7,8,9,\n        10,11,12,13,14,15,16,17,18,19\n    );\n\n    @Override\n    protected Iterator<Integer> doPageRead() {\n      int start = page * pageSize;\n      int end = Math.min(start + pageSize, data.size());\n      return data.subList(start, end).iterator();\n    }\n  }\n\n  @Test\n  void jumpToItem_shouldReadExactItem_afterJump() throws Exception {\n    PaginatedDataItemReader reader = new PaginatedDataItemReader();\n    reader.open(new ExecutionContext());\n\n    reader.jumpToItem(7);\n\n    Integer value = reader.read();\n    assertEquals(7, value);\n  }\n\n  @Test\n  void jumpToItem_zeroIndex() throws Exception {\n    PaginatedDataItemReader reader = new PaginatedDataItemReader();\n    reader.open(new ExecutionContext());\n\n    reader.jumpToItem(0);\n\n    Integer value = reader.read();\n    assertEquals(0, value);\n  }\n\n  @Test\n  void jumpToItem_lastItemInPage() throws Exception {\n    PaginatedDataItemReader reader = new PaginatedDataItemReader();\n    reader.open(new ExecutionContext());\n\n    reader.jumpToItem(9);\n\n    Integer value = reader.read();\n    assertEquals(9, value);\n  }\n\n  @Test\n  void jumpToItem_firstItemOfNextPage() throws Exception {\n    PaginatedDataItemReader reader = new PaginatedDataItemReader();\n    reader.open(new ExecutionContext());\n\n    reader.jumpToItem(10);\n\n    Integer value = reader.read();\n    assertEquals(10, value);\n  }\n\n}\n```",
    "labels": [
      "in: infrastructure",
      "type: bug",
      "for: backport-to-5.2.x"
    ],
    "comments": [],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5161,
    "title": "OptimisticLockingFailureException in JobRepositoryTestUtils.removeJobExecutions() since Spring Batch 5.2.3",
    "state": "open",
    "created_at": "2025-12-11T22:38:48Z",
    "updated_at": "2026-01-13T11:24:47Z",
    "author": "szopal24",
    "url": "https://github.com/spring-projects/spring-batch/issues/5161",
    "body": "Since Spring Batch 5.2.3, calling JobRepositoryTestUtils.removeJobExecutions() in test cleanup methods throws OptimisticLockingFailureException when trying to delete job executions.\n\nEnvironment\nSpring Batch: 5.2.3, 5.2.4\nSpring Boot: 3.4.5, 3.5.8\nJava: 17\nDatabase: PostgreSQL (with table prefix BOOT3_BATCH_)\n\n**Test Code Example:**\n```\n\n    @Test\n    public void testJob() throws Exception {\n        JobExecution jobExecution = jobLauncherTestUtils.launchJob(defaultJobParameters());\n        jobExecutionList.add(jobExecution);\n        \n        assertThat(jobExecution.getExitStatus()).isEqualTo(ExitStatus.COMPLETED);\n    }\n\n    @After\n    public void cleanUp() {\n        // This throws OptimisticLockingFailureException since 5.2.3\n        jobRepositoryTestUtils.removeJobExecutions(jobExecutionList);\n    }\n```\n\nResult:\n\n```\norg.springframework.dao.OptimisticLockingFailureException: Attempt to delete step execution id=95106 with wrong version (1)\n\n\tat org.springframework.batch.core.repository.dao.JdbcStepExecutionDao.deleteStepExecution(JdbcStepExecutionDao.java:386)\n\tat org.springframework.batch.core.repository.support.SimpleJobRepository.deleteStepExecution(SimpleJobRepository.java:316)\n\tat org.springframework.batch.core.repository.support.SimpleJobRepository.deleteJobExecution(SimpleJobRepository.java:324)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:360)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:196)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163)\n\tat org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:380)\n\tat org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\n\tat org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223)\n\tat jdk.proxy2/jdk.proxy2.$Proxy126.deleteJobExecution(Unknown Source)\n\tat org.springframework.batch.test.JobRepositoryTestUtils.removeJobExecution(JobRepositoryTestUtils.java:156)\n\tat org.springframework.batch.test.JobRepositoryTestUtils.removeJobExecutions(JobRepositoryTestUtils.java:138)\n\tat xx.yyyyy.xx.aaaa.vvvv.bbbb.wwwww.SpringBatchIntegrationTest.cleanUp(SpringBatchIntegrationTest.java:82)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)\n\tat org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)\n\tat org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:252)\n\tat org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:97)\n\tat org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)\n\tat org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)\n\tat org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:191)\n```\n\nThis breaks all existing Spring Batch integration tests that use JobRepositoryTestUtils.removeJobExecutions() in cleanup methods. This is a breaking change that affects any project upgrading from 5.2.2 to 5.2.3+. The Spring Batch documentation and Javadocs for JobRepositoryTestUtils.removeJobExecutions() do not mention this breaking change or provide guidance on how to update existing tests.",
    "labels": [
      "in: test",
      "type: bug",
      "for: backport-to-5.2.x"
    ],
    "comments": [
      {
        "author": "quaff",
        "created_at": "2025-12-12T00:51:10Z",
        "body": "It's introduced by #4793, you should query each `JobExecution` from `jobExecutionList` to get latest version for delete."
      },
      {
        "author": "szopal24",
        "created_at": "2025-12-17T09:44:35Z",
        "body": "Thank you, I managed to resolve the problem. Unfortunately, this means I now have to apply changes in more than 100 Spring Batch applications that were using this method to clean up the BATCH_JOB_EXECUTION table. While I understand the reasoning behind the change, it does have a significant impact on existing test setups."
      },
      {
        "author": "quaff",
        "created_at": "2025-12-18T02:03:28Z",
        "body": "@szopal24 I created #5173 to fix it."
      },
      {
        "author": "szopal24",
        "created_at": "2025-12-18T07:23:47Z",
        "body": "Thank you!"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T11:24:35Z",
        "body": "@szopal24  Thank you for reporting this issue and thank you @quaff for the PR.\n\nI will plan the fix in 6.0.2 and back port it to 5.2.5"
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": [
        "12b16b32adbbf35ead57b5e3b8d0ec84c56789ec"
      ]
    }
  },
  {
    "number": 5166,
    "title": "Wrong database migration to spring-batch 6.x for DB2LUW",
    "state": "open",
    "created_at": "2025-12-15T15:23:25Z",
    "updated_at": "2026-01-13T11:16:59Z",
    "author": "bekoenig",
    "url": "https://github.com/spring-projects/spring-batch/issues/5166",
    "body": "Hi,\n\n[this migration](https://github.com/spring-projects/spring-batch/blob/98c10cd981b5f4ddc65e7071f6a603a3781514fd/spring-batch-core/src/main/resources/org/springframework/batch/core/migration/6.0/migration-db2.sql#L2) is only applicable to Informix servers. IBM DB2 LUW (10.1+) does not support sequence renaming (see [reference](https://www.ibm.com/docs/en/db2/12.1.x?topic=statements-rename)).\n\nTo address this lack of support, we implemented a Java-based migration that retrieves the last sequence number and creates a new sequence starting from this value.\n\nBest regards,\nBen",
    "labels": [
      "type: bug",
      "in: core"
    ],
    "comments": [
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T11:16:27Z",
        "body": "Thank you for reporting this!\n\n> To address this lack of support, we implemented a Java-based migration that retrieves the last sequence number and creates a new sequence starting from this value.\n\nIndeed, if IBM DB2 LUW (10.1+) does not support sequence renaming, then users should find a way to create a new sequence starting from the last value of the previous one. I will add a note about that in the migration script."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5178,
    "title": "Add ZonedDateTime and OffsetDateTime support to JobParametersConverter",
    "state": "open",
    "created_at": "2025-12-21T05:28:47Z",
    "updated_at": "2026-01-13T10:52:20Z",
    "author": "thswlsqls",
    "url": "https://github.com/spring-projects/spring-batch/issues/5178",
    "body": "**Expected Behavior**\n\n`ZonedDateTime` and `OffsetDateTime` should be supported as JobParameters types, similar to `LocalDateTime`, `LocalDate`, and `LocalTime`.\n\nExample usage:\n```java\nZonedDateTime scheduleTime = ZonedDateTime.of(2023, 12, 25, 10, 30, 0, 0, ZoneId.of(\"Asia/Seoul\"));\nJobParameters parameters = new JobParametersBuilder()\n    .addJobParameter(\"schedule.time\", scheduleTime, ZonedDateTime.class, true)\n    .toJobParameters();\n```\n\n**Current Behavior**\n\nSpring Batch currently only provides converters for `LocalDateTime`, `LocalDate`, and `LocalTime`. \n`ZonedDateTime` and `OffsetDateTime` cannot be used as JobParameters because there are no converters available.\n\n**Context**\n\n**How has this issue affected you?**\nWhen working with global services or multi-timezone applications, we need to pass timezone-aware date/time values as JobParameters, but currently only timezone-naive types (`LocalDateTime`, `LocalDate`, `LocalTime`) are supported.\n\n**What are you trying to accomplish?**\n- Execute batch jobs based on specific timezones in global services\n- Require both UTC and local timezone in log analysis\n- Include timezone information for each country in multi-country services\n\n**What other alternatives have you considered?**\n- Converting to `LocalDateTime` and storing timezone separately (loses timezone information)\n- Using `String` type and parsing manually (error-prone, not type-safe)\n- Using `Date` with timezone offset (legacy API, not recommended)\n\n**Are you aware of any workarounds?**\nCurrently, there is no clean workaround. Users must convert to `LocalDateTime` and lose timezone information, or use `String` type which is not type-safe.\n\n**Proposed Implementation:**\n- Add `ZonedDateTimeToStringConverter` and `StringToZonedDateTimeConverter`\n- Add `OffsetDateTimeToStringConverter` and `StringToOffsetDateTimeConverter`\n- Register new converters in `DefaultJobParametersConverter`\n- Add related test code",
    "labels": [
      "type: feature",
      "in: core",
      "for: backport-to-5.2.x"
    ],
    "comments": [
      {
        "author": "scordio",
        "created_at": "2025-12-21T09:40:42Z",
        "body": "> Currently, there is no clean workaround. Users must convert to `LocalDateTime` and lose timezone information, or use `String` type which is not type-safe.  \n\nThat's not entirely true. In an average Spring Boot application, this conversion capability could be obtained out of the box when defining a [`DefaultFormattingConversionService`](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/format/support/DefaultFormattingConversionService.html) bean in the Spring context:\n\n```java\nimport org.springframework.format.support.DefaultFormattingConversionService;\n\n@Bean\nDefaultFormattingConversionService conversionService() {\n  return new DefaultFormattingConversionService();\n}\n```\n\nThis allows the use of job parameters like the following:\n\n```java\nimport org.springframework.format.annotation.DateTimeFormat;\nimport org.springframework.format.annotation.DateTimeFormat.ISO;\n\n@Bean\n@StepScope\nItemReader<Item> itemReader(@Value(\"#{jobParameters['targetDate']}\") @DateTimeFormat(iso = ISO.DATE) LocalDate targetDate) {\n  ...\n}\n```\n\nThe same should also work with [`ZonedDateTime`](https://github.com/spring-projects/spring-framework/blob/0b2bb7e751d5effd798adaf545c64a7342657ecc/spring-context/src/main/java/org/springframework/format/datetime/standard/DateTimeFormatterRegistrar.java#L180-L182) and [`OffsetDateTime`](https://github.com/spring-projects/spring-framework/blob/0b2bb7e751d5effd798adaf545c64a7342657ecc/spring-context/src/main/java/org/springframework/format/datetime/standard/DateTimeFormatterRegistrar.java#L184-L186).\n\nNevertheless, it would be nice if Spring Batch would offer this out of the box.\n \n> * Register new converters in `DefaultJobParametersConverter`\n\nAs Spring Batch already depends on `spring-context`, what about instantiating a `DefaultFormattingConversionService` instead of `DefaultConversionService` in the `DefaultJobParametersConverter` constructor?\n\nhttps://github.com/spring-projects/spring-batch/blob/2cc7890be100034f66bab9b4297de93dfbfad3a3/spring-batch-core/src/main/java/org/springframework/batch/core/converter/DefaultJobParametersConverter.java#L79\n\nSome existing custom converters in Spring Batch might also become obsolete."
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T10:48:23Z",
        "body": "@thswlsqls Thank you for opening this issue and contributing a PR!\n\n@scordio Thank you for the follow up and for the PR as well!\n\nBoth PRs LGTM ðŸ‘  I think we can merge #5179 for 6.0.2 and then #5186 in 6.1.0 so that users don't have to wait a year or more to get these two converters (and indeed, it's better to leverage converters from Spring Framework as in #5186). "
      },
      {
        "author": "scordio",
        "created_at": "2026-01-13T10:52:20Z",
        "body": "I'll rebase #5186 once #5179 is merged."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": [
        "077a33238b8990e6993fb29a35dc9204b315a339"
      ]
    }
  },
  {
    "number": 5181,
    "title": "MetaDataInstanceFactory default values cause StepContext collision in StepScopeTestUtils when @SpringBatchTest is active",
    "state": "open",
    "created_at": "2025-12-23T07:20:59Z",
    "updated_at": "2026-01-13T10:21:20Z",
    "author": "KILL9-NO-MERCY",
    "url": "https://github.com/spring-projects/spring-batch/issues/5181",
    "body": "## Bug description: \nThere is a logical collision in StepSynchronizationManager when using StepScopeTestUtils in a test environment managed by @SpringBatchTest.\n\nStepExecution determines equality based on stepName, jobExecutionId, and id. Since MetaDataInstanceFactory provides static default values for all these fields, multiple instances created by the factory are treated as identical keys in the SynchronizationManagerSupport.contexts map.\n\nThis prevents StepScopeTestUtils from registering a new context with custom JobParameters, as the computeIfAbsent logic finds the existing context registered by StepScopeTestExecutionListener (which is part of @SpringBatchTest).\n\n## Steps to reproduce:\nAnnotate a test class with @SpringBatchTest.\n\nInside a test method, use StepScopeTestUtils.doInStepScope() with a StepExecution created via MetaDataInstanceFactory.createStepExecution(jobParameters).\n\nThe Tasklet or ItemStream inside the scope will fail to see the jobParameters because it is bound to the listener's initial context.\n\n## Failing Example: \nexample job\n```java\n@Slf4j\n@Configuration\npublic class IssueReproductionJobConfiguration {\n    @Bean\n    public Job issueReproductionJob(JobRepository jobRepository, Step issueReproductionStep) {\n        return new JobBuilder(jobRepository)\n                .incrementer(new RunIdIncrementer())\n                .start(issueReproductionStep)\n                .build();\n    }\n\n    @Bean\n    public Step issueReproductionStep(\n            JobRepository jobRepository,\n            Tasklet issueReproductionTasklet\n    ) {\n        return new StepBuilder(jobRepository)\n                .tasklet(issueReproductionTasklet)\n                .build();\n    }\n\n    @Bean\n    @StepScope\n    public Tasklet issueReproductionTasklet(@Value(\"#{jobParameters['testParam']}\") String testParam) {\n        return (contribution, chunkContext) -> {\n            contribution.getStepExecution().getExecutionContext().putString(\"result\", testParam);\n            return RepeatStatus.FINISHED;\n        };\n    }\n}\n```\n\ntest class\n```java\n@SpringBatchTest\n@SpringBootTest\n@ActiveProfiles(\"test\")\n@Import(TestBatchConfiguration.class)\npublic class IssueReproductionTest {\n    @Autowired\n    private Tasklet issueReproductionTasklet;\n\n    public StepExecution getStepExecution() throws IOException {\n        return MetaDataInstanceFactory.createStepExecution(\"dummy\", -1L);\n    }\n\n    @Test\n    @DisplayName(\"MetadataInstanceFactory ID collision causes JobParameter injection failure\")\n    void reproduceIdCollisionBug() throws Exception {\n        // Given\n        String expectedValue = \"HelloBatch\";\n        JobParameters jobParameters = new JobParametersBuilder()\n                .addString(\"testParam\", expectedValue)\n                .toJobParameters();\n\n        // MetadataInstanceFactory in 6.x / maybe after 5.2.3?? creates StepExecution with fixed ID 1234L\n        StepExecution stepExecution = MetaDataInstanceFactory.createStepExecution(jobParameters);\n\n        // When\n        StepScopeTestUtils.doInStepScope(stepExecution, () ->\n                Objects.requireNonNull(issueReproductionTasklet.execute(stepExecution.createStepContribution(), null))\n        );\n\n        // Then\n        String actualValue = stepExecution.getExecutionContext().getString(\"result\");\n\n        // This will FAIL because 'actualValue' will be null.\n        // The Tasklet retrieved the listener's context (which has no JobParameters)\n        // instead of the one passed via StepScopeTestUtils due to ID collision (1234L).\n        assertEquals(expectedValue, actualValue);\n    }\n}\n\n@TestConfiguration\npublic class TestBatchConfiguration extends DefaultBatchConfiguration {\n}\n```\n\napplication-test.yml\n```yaml\nspring:\n  batch:\n    job:\n      enabled: false\n```\ntest result:\n```bash\nValue for key=[result] is not of type: [class java.lang.String], it is [null]\njava.lang.ClassCastException: Value for key=[result] is not of type: [class java.lang.String], it is [null]\n```\n\n## Expected behavior:\nThe StepExecution and its corresponding StepContext created within StepScopeTestUtils.doInStepScope() should be correctly registered and accessible through the StepSynchronizationManager, even when @SpringBatchTest is active.\n\n(Note: Deciding on the best fix seems non-trivial to me, as it could involve changing the ID generation strategy in MetaDataInstanceFactory or adjusting how StepSynchronizationManager handles overlapping registrations in a test environment.)\nWorkaround: Users must manually provide a unique name or ID to bypass the equals/hashCode collision:\n\n## Workaround: \nTo bypass the current collision, users can explicitly define a getStepExecution() method within their test class. By returning a StepExecution with a unique name or a different ID (e.g., -1L), you can prevent the StepScopeTestExecutionListener from occupying the default ID (1234L), thus allowing StepScopeTestUtils to work as intended:\n\n```java\n/**\n * Workaround: Define getStepExecution() in the test class to avoid ID collision.\n * By providing a non-default ID or name, we ensure that the listener-registered \n * context does not conflict with the one created in StepScopeTestUtils.\n */\npublic StepExecution getStepExecution() {\n    return MetaDataInstanceFactory.createStepExecution(\"uniqueStep\", -1L);\n}\n```\n\ntest result:\n```bash\n> Task :test\nBUILD SUCCESSFUL in 3s\n```\n\nThanks for your time and for maintaining this great project!",
    "labels": [
      "in: test",
      "status: waiting-for-reporter",
      "type: bug"
    ],
    "comments": [
      {
        "author": "injae-kim",
        "created_at": "2026-01-11T11:25:50Z",
        "body": "FYI) Fix PR: https://github.com/spring-projects/spring-batch/pull/5208 ðŸ‘"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T10:21:04Z",
        "body": "I am trying to reproduce this issue but I am not able to. The test you shared uses Spring Boot, but I want to make sure this is a valid issue by only using Spring Batch first.\n\nAt 9ae777572a0978572e25f04d4cb93c0ad02b9a0f, when I add the following classes (the same you shared but without Spring Boot) in the `org.springframework.batch.test` package, the test you mentioned passes:\n\n```java\npackage org.springframework.batch.test;\n\nimport org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;\nimport org.springframework.batch.core.configuration.annotation.StepScope;\nimport org.springframework.batch.core.job.Job;\nimport org.springframework.batch.core.job.builder.JobBuilder;\nimport org.springframework.batch.core.job.parameters.RunIdIncrementer;\nimport org.springframework.batch.core.repository.JobRepository;\nimport org.springframework.batch.core.step.Step;\nimport org.springframework.batch.core.step.builder.StepBuilder;\nimport org.springframework.batch.core.step.tasklet.Tasklet;\nimport org.springframework.batch.infrastructure.repeat.RepeatStatus;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\n@EnableBatchProcessing\npublic class IssueReproductionJobConfiguration {\n\n    @Bean\n    public Job issueReproductionJob(JobRepository jobRepository, Step issueReproductionStep) {\n        return new JobBuilder(jobRepository)\n                .incrementer(new RunIdIncrementer())\n                .start(issueReproductionStep)\n                .build();\n    }\n\n    @Bean\n    public Step issueReproductionStep(\n            JobRepository jobRepository,\n            Tasklet issueReproductionTasklet\n    ) {\n        return new StepBuilder(jobRepository)\n                .tasklet(issueReproductionTasklet)\n                .build();\n    }\n\n    @Bean\n    @StepScope\n    public Tasklet issueReproductionTasklet(@Value(\"#{jobParameters['testParam']}\") String testParam) {\n        return (contribution, chunkContext) -> {\n            contribution.getStepExecution().getExecutionContext().putString(\"result\", testParam);\n            return RepeatStatus.FINISHED;\n        };\n    }\n}\n```\n\n```java\npackage org.springframework.batch.test;\n\nimport java.io.IOException;\nimport java.util.Objects;\n\nimport org.junit.jupiter.api.DisplayName;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.extension.ExtendWith;\n\nimport org.springframework.batch.core.job.parameters.JobParameters;\nimport org.springframework.batch.core.job.parameters.JobParametersBuilder;\nimport org.springframework.batch.core.step.StepExecution;\nimport org.springframework.batch.core.step.tasklet.Tasklet;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.test.context.ContextConfiguration;\nimport org.springframework.test.context.junit.jupiter.SpringExtension;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\n\n@ContextConfiguration(classes = IssueReproductionJobConfiguration.class)\n@ExtendWith(SpringExtension.class)\npublic class IssueReproductionTest {\n\n    @Autowired\n    private Tasklet issueReproductionTasklet;\n\n    public StepExecution getStepExecution() throws IOException {\n        return MetaDataInstanceFactory.createStepExecution(\"dummy\", -1L);\n    }\n\n    @Test\n    @DisplayName(\"MetadataInstanceFactory ID collision causes JobParameter injection failure\")\n    void reproduceIdCollisionBug() throws Exception {\n        // Given\n        String expectedValue = \"HelloBatch\";\n        JobParameters jobParameters = new JobParametersBuilder()\n                .addString(\"testParam\", expectedValue)\n                .toJobParameters();\n\n        // MetadataInstanceFactory in 6.x / maybe after 5.2.3?? creates StepExecution with fixed ID 1234L\n        StepExecution stepExecution = MetaDataInstanceFactory.createStepExecution(jobParameters);\n\n        // When\n        StepScopeTestUtils.doInStepScope(stepExecution, () ->\n                Objects.requireNonNull(issueReproductionTasklet.execute(stepExecution.createStepContribution(), null))\n        );\n\n        // Then\n        String actualValue = stepExecution.getExecutionContext().getString(\"result\");\n\n        // This will FAIL because 'actualValue' will be null.\n        // The Tasklet retrieved the listener's context (which has no JobParameters)\n        // instead of the one passed via StepScopeTestUtils due to ID collision (1234L).\n        assertEquals(expectedValue, actualValue);\n    }\n}\n```\n\nCan you please check?"
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5182,
    "title": "ChunkOrientedStep updates ExecutionContext even when a chunk fails, leading to data loss on restart",
    "state": "open",
    "created_at": "2025-12-23T07:33:23Z",
    "updated_at": "2026-01-14T06:10:40Z",
    "author": "KILL9-NO-MERCY",
    "url": "https://github.com/spring-projects/spring-batch/issues/5182",
    "body": "Hello Spring Batch Team!\n\n## Bug description: \nIn Spring Batch 6.x, the newly introduced ChunkOrientedStep calls itemStream.update() and jobRepository.updateExecutionContext() within a finally block in both processChunkSequentially and processChunkConcurrently. Unlike the traditional TaskletStep implementation.\n\nThis causes the ItemStream state (e.g., read count, current index) to be persisted even when a chunk transaction fails and rolls back. Consequently, upon restart, the step resumes from the \"failed\" offset, leading to silent data loss of the records within the failed chunk.\n\n\n## Code Comparison (The Root Cause)\n\n#### Spring Batch 5.x (TaskletStep.java)\nIn version 5, the state is updated only after the chunk is successfully processed and committed.\n\n```java\n\n// TaskletStep.java (Line 452)\n// This logic is inside the successful processing flow\nstream.update(stepExecution.getExecutionContext());\ngetJobRepository().updateExecutionContext(stepExecution);\nstepExecution.incrementCommitCount();\n```\n\n\n#### Spring Batch 6.x (ChunkOrientedStep.java)\nIn version 6, the update logic was moved to a finally block, forcing the update even during a rollback.\n```java\n// ChunkOrientedStep.java\nprivate void processChunkSequentially(...) {\n    try {\n        // chunk read/process/write logic\n    } catch (Exception e) {\n        // exception handling\n        throw e;\n    } finally {\n        // BUG: Always executed even if the transaction is rolled back!\n        this.compositeItemStream.update(stepExecution.getExecutionContext());\n        getJobRepository().updateExecutionContext(stepExecution);\n    }\n}\n```\n\n## Impact\nTransaction Inconsistency: The business data is rolled back, but the Batch Metadata (index/offset) is committed/updated.\n\nData Loss: On restart, the ItemReader resumes from the position after the failed chunk, meaning the records in the failed chunk are never re-processed.\n\n## Environment\nSpring Batch version: 6.0.1\nComponents: ChunkOrientedStep \n\n## Expected behavior\nExecutionContext and ItemStream state should only be updated if the chunk transaction is successful. If an exception occurs, the finally block should not persist the advanced state to the JobRepository.\n\n\n## Suggested Fix\nThe state update logic should be moved from the finally block of processChunkXXX methods to the doExecute method, specifically after the transaction has successfully completed.\n\nProposed change in ChunkOrientedStep.java:\n```java\n@Override\nprotected void doExecute(StepExecution stepExecution) throws Exception {\n    stepExecution.getExecutionContext().put(STEP_TYPE_KEY, this.getClass().getName());\n    \n    while (this.chunkTracker.get().moreItems() && !interrupted(stepExecution)) {\n       // process next chunk in its own transaction\n       this.transactionTemplate.executeWithoutResult(transactionStatus -> {\n          // process next chunk\n       });\n       getJobRepository().update(stepExecution);\n       \n       // FIX: Update ItemStream and ExecutionContext ONLY after successful transaction commit\n       this.compositeItemStream.update(stepExecution.getExecutionContext());\n       getJobRepository().updateExecutionContext(stepExecution);\n    }\n}\n```\nNote: The corresponding update calls inside processChunkSequentially and processChunkConcurrently's finally blocks must be removed to prevent duplicate or premature updates.\n\n\nThanks for your time and for maintaining this great project! If you need more details or sample please tell me!",
    "labels": [
      "type: bug",
      "in: core"
    ],
    "comments": [
      {
        "author": "KILL9-NO-MERCY",
        "created_at": "2026-01-06T13:09:06Z",
        "body": "Hi Spring Batch team!\nI wanted to add some follow-up context related to this issue.\n\nI recently opened another issue (#5199) regarding the transaction boundary of JobRepository.update(stepExecution) in ChunkOrientedStep#doExecute.\n\nIf the fix proposed in #5199 is applied (moving JobRepository.update(stepExecution) inside the chunk transaction), then the proposed fix in this issue (#5182) should be slightly adjusted as well.\n\nthe past proposed fix suggests moving those updates to doExecute, after the transaction completes. but If `JobRepository.update(stepExecution)` itself is moved inside the transaction (as proposed in #5199), then to preserve full consistency, the following operations should also be aligned with the same transaction boundary\n- JobRepository.update(stepExecution)\n- ItemStream.update(stepExecution.getExecutionContext())\n- JobRepository.updateExecutionContext(stepExecution)\n\n## Suggested alignment\n```java\n@Override\nprotected void doExecute(StepExecution stepExecution) throws Exception {\n    stepExecution.getExecutionContext().put(STEP_TYPE_KEY, this.getClass().getName());\n    \n    while (this.chunkTracker.get().moreItems() && !interrupted(stepExecution)) {\n       // process next chunk in its own transaction\n       this.transactionTemplate.executeWithoutResult(transactionStatus -> {\n           processNextChunk(transactionStatus, contribution, stepExecution);\n           // FIX: Update ItemStream and ExecutionContext\n           this.compositeItemStream.update(stepExecution.getExecutionContext());\n           getJobRepository().updateExecutionContext(stepExecution);\n           // FIX #5199\n           getJobRepository().update(stepExecution);\n       });\n    }\n}\n```\n\nJust wanted to point this out so both issues can be addressed consistently. Happy to help with a unified fix or a test if needed. Thanks again!"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-12T18:51:55Z",
        "body": "Thank you for your continuous feedback on v6.\n\n> Happy to help with a unified fix or a test if needed.\n\nYes please. That would be helpful! Many thanks upfront.\n\nEDIT: It looks like we have a PR for this issue: #5195. Can you please check if it suggests the same/similar fix that you have in mind?"
      },
      {
        "author": "KILL9-NO-MERCY",
        "created_at": "2026-01-14T06:10:40Z",
        "body": "Thanks for the information! I'll check PR #5195 and leave a comment there."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5183,
    "title": "ScopeNotActiveException with @StepScope ItemProcessor in Multi-threaded ChunkOrientedStep",
    "state": "open",
    "created_at": "2025-12-23T07:46:16Z",
    "updated_at": "2026-01-14T08:57:39Z",
    "author": "KILL9-NO-MERCY",
    "url": "https://github.com/spring-projects/spring-batch/issues/5183",
    "body": "Hello Spring Batch Team,\n\nI am reporting an issue regarding the new ChunkOrientedStep introduced in version 6.0. It appears that when a step is configured as multi-threaded, an ItemProcessor defined with @StepScope fails to resolve correctly within the worker threads.\n\n## Bug Description\nIn the ChunkOrientedStep implementation, specifically when using processChunkConcurrently, the StepContext does not seem to be propagated to the worker threads managed by the TaskExecutor.\n\nAs a result, when the worker thread attempts to invoke the ItemProcessor (which is a @StepScope proxy), it throws a ScopeNotActiveException because the StepSynchronizationManager on that specific thread has no active context.\n\n## Environment\nSpring Batch version: v6\nStep Implementation: ChunkOrientedStep\nConfiguration: TaskExecutor (e.g., SimpleAsyncTaskExecutor) + @StepScope ItemProcessor\n\n## Reproducible Configuration\n```java\n@Bean\npublic Step issueReproductionStep(\n        JobRepository jobRepository,\n        ItemReader<TestItem> reader,\n        ItemProcessor<TestItem, TestItem> itemProcessor, // @StepScope Bean\n        ItemWriter<TestItem> writer\n) {\n    return new StepBuilder(jobRepository)\n            .<TestItem, TestItem>chunk(1)\n            .reader(reader)\n            .processor(itemProcessor)\n            .writer(writer)\n            .taskExecutor(new SimpleAsyncTaskExecutor()) // Multi-threading enabled\n            .build();\n}\n\n@Bean\n@StepScope\npublic ItemProcessor<TestItem, TestItem> issueReproductionProcessor() {\n    return item -> {\n        log.info(\"[Thread: {}] Processing item: {}\", Thread.currentThread().getName(), item.getName());\n        return item;\n    };\n}\n```\n\n## Actual Result (Stacktrace)\nThe error occurs when the worker thread tries to access the scoped ItemProcessor:\n```bash\nCaused by: org.springframework.beans.factory.support.ScopeNotActiveException: Error creating bean with name 'scopedTarget.issueReproductionProcessor': Scope 'step' is not active for the current thread\n    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:381)\n    ...\n    at jdk.proxy2/jdk.proxy2.$Proxy134.process(Unknown Source)\n    at org.springframework.batch.core.step.item.ChunkOrientedStep.doProcess(ChunkOrientedStep.java:655)\n    ...\nCaused by: java.lang.IllegalStateException: No context holder available for step scope\n    at org.springframework.batch.core.scope.StepScope.getContext(StepScope.java:167)\n```\n\n## Expected Behavior\nI am not certain whether this is an intended architectural change or an oversight in the new implementation. However, if this is a bug, the @StepScope ItemProcessor should function correctly within worker threads, as it did in previous versions.\n\n\n## Proposed change in ChunkOrientedStep.processChunkConcurrently:\n```java\n// Inside processChunkConcurrently method\nFuture<O> itemProcessingFuture = this.taskExecutor.submit(() -> {\n    try {\n        // Register step execution to the current worker thread's StepSynchronizationManager\n        StepSynchronizationManager.register(stepExecution);\n        return processItem(item, contribution);\n    } finally {\n        // Clear the context after processing to prevent memory leaks\n        StepSynchronizationManager.close();\n    }\n});\n```\n\nThanks for your time and for maintaining this project! Please let me know if you need any further information or a working reproduction repository!",
    "labels": [
      "type: bug",
      "in: core"
    ],
    "comments": [
      {
        "author": "LeeHyungGeol",
        "created_at": "2026-01-07T03:23:56Z",
        "body": "Hello @fmbenhassine.\n\nWould it be okay if i give it a try on this issue?"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T09:53:35Z",
        "body": "@KILL9-NO-MERCY Thank you for reporting this issue!\n\n> I am not certain whether this is an intended architectural change or an oversight in the new implementation.\n\nThis is an oversight in the new implementation. In fact `org.springframework.batch.core.step.item.ChunkOrientedStepIntegrationTests#testConcurrentChunkOrientedStepSuccess` fails when [this item processor](https://github.com/spring-projects/spring-batch/blob/a6a53c46fca3aa920f4f07ac7ddbf39493081f66/spring-batch-core/src/test/java/org/springframework/batch/core/step/item/TestConfiguration.java#L56) is step-scoped. The suggested change LGTM (with it, the test passes with a step-scoped item processor). Thank you for the suggestion.\n\n@LeeHyungGeol Sure! Thank you for your offer to help ðŸ™ You are welcome to contribute a PR with the suggested change here and making the item processor that I mentioned earlier step-scoped. I will plan the fix for the upcoming 6.0.2."
      },
      {
        "author": "LeeHyungGeol",
        "created_at": "2026-01-14T08:39:07Z",
        "body": " @fmbenhassine Thank you for the confirmation!\n\n  I'll work on a PR with the suggested fix and update the integration test\n  to use a step-scoped item processor.\n\n  Could you please assign this issue to me?"
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5191,
    "title": "Jackson2ExecutionContextStringSerializer fails to serialize job parameters with JobStep",
    "state": "closed",
    "created_at": "2025-12-30T15:33:54Z",
    "updated_at": "2026-01-14T10:26:18Z",
    "author": "andrianov17",
    "url": "https://github.com/spring-projects/spring-batch/issues/5191",
    "body": "**Bug description**\nAfter upgrade from Spring Batch 5.2.3 to Spring Batch 6.0.1 and preserving previous org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer serializer, JobStep fails with the exception:\n\n```\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: Can not write a field name, expecting a value (through reference chain: java.util.HashMap[\"org.springframework.batch.core.step.job.JobStep.JOB_PARAMETERS\"]->org.springframework.batch.core.job.parameters.JobParameters[\"parameters\"]->java.util.Collections$UnmodifiableSet[0])\n\tat com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:400)\n\tat com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:371)\n\tat com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:346)\n\tat com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContentsUsing(CollectionSerializer.java:186)\n\tat com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:120)\n\tat com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:25)\n\tat com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serializeWithType(AsArraySerializerBase.java:265)\n\tat com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:734)\n\tat com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:760)\n\tat com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:643)\n\tat com.fasterxml.jackson.databind.ser.std.MapSerializer.serializeTypedFields(MapSerializer.java:1026)\n\tat com.fasterxml.jackson.databind.ser.std.MapSerializer.serializeFields(MapSerializer.java:778)\n\tat com.fasterxml.jackson.databind.ser.std.MapSerializer.serializeWithoutTypeInfo(MapSerializer.java:763)\n\tat com.fasterxml.jackson.databind.ser.std.MapSerializer.serializeWithType(MapSerializer.java:732)\n\tat com.fasterxml.jackson.databind.ser.std.MapSerializer.serializeWithType(MapSerializer.java:34)\n\tat com.fasterxml.jackson.databind.ser.impl.TypeWrappedSerializer.serialize(TypeWrappedSerializer.java:32)\n\tat com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:503)\n\tat com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:342)\n\tat com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4926)\n\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:4105)\n\tat org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer.serialize(Jackson2ExecutionContextStringSerializer.java:165)\n\tat org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer.serialize(Jackson2ExecutionContextStringSerializer.java:114)\n\tat org.springframework.batch.core.repository.dao.jdbc.JdbcExecutionContextDao.serializeContext(JdbcExecutionContextDao.java:361)\n\t... 28 more\nCaused by: com.fasterxml.jackson.core.JsonGenerationException: Can not write a field name, expecting a value\n\tat com.fasterxml.jackson.core.JsonGenerator._constructWriteException(JsonGenerator.java:2937)\n\tat com.fasterxml.jackson.core.JsonGenerator._reportError(JsonGenerator.java:2921)\n\tat com.fasterxml.jackson.core.json.UTF8JsonGenerator.writeFieldName(UTF8JsonGenerator.java:217)\n\tat org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer$JobParametersModule$JobParameterSerializer.serialize(Jackson2ExecutionContextStringSerializer.java:213)\n\tat org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer$JobParametersModule$JobParameterSerializer.serialize(Jackson2ExecutionContextStringSerializer.java:195)\n\tat com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContentsUsing(CollectionSerializer.java:179)\n\t... 49 more\n```\n\nUpon debugging, it fails trying to serialize the first JobParameter.\n\n**Environment**\nSpring Batch 6.0.1, SQL Server 2022\n\n**Steps to reproduce**\n- Have a job using JobStep step\n- Run the job passing some parameters to it\n\n**Expected behavior**\nJob runs successfully, saving step execution context like this (as it was in 5.2.3):\n```\n{\n\t\"@class\": \"java.util.HashMap\",\n\t\"childJobExecId\": [\n\t\t\"java.lang.Long\",\n\t\t3480\n\t],\n\t\"org.springframework.batch.core.step.job.JobStep.JOB_PARAMETERS\": {\n\t\t\"@class\": \"org.springframework.batch.core.JobParameters\",\n\t\t\"parameters\": {\n\t\t\t\"@class\": \"java.util.Collections$UnmodifiableMap\",\n\t\t\t\"queueItemId\": {\n\t\t\t\t\"@class\": \"org.springframework.batch.core.JobParameter\",\n\t\t\t\t\"value\": \"250702\",\n\t\t\t\t\"type\": \"java.lang.String\",\n\t\t\t\t\"identifying\": false\n\t\t\t},\n\t\t\t\"execType\": {\n\t\t\t\t\"@class\": \"org.springframework.batch.core.JobParameter\",\n\t\t\t\t\"value\": \"MANUAL\",\n\t\t\t\t\"type\": \"java.lang.String\",\n\t\t\t\t\"identifying\": false\n\t\t\t},\n\t\t\t\"user\": {\n\t\t\t\t\"@class\": \"org.springframework.batch.core.JobParameter\",\n\t\t\t\t\"value\": \"system\",\n\t\t\t\t\"type\": \"java.lang.String\",\n\t\t\t\t\"identifying\": false\n\t\t\t}\n\t\t}\n\t},\n\t\"batch.version\": \"5.2.3\"\n}\n```\n\nPlease also notice that org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer.JobParametersModule.JobParameterSerializer#serialize is not adjusted to serialize parameter name.\n\n**Minimal Complete Reproducible example**\nPretty straightforward - see steps to reproduce above\n",
    "labels": [
      "type: bug",
      "in: core"
    ],
    "comments": [
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-12T20:00:45Z",
        "body": "Thank you for opening this issue. If you upgrade to v6, you should be using `JacksonExecutionContextStringSerializer` and not `Jackson2ExecutionContextStringSerializer`. Did you update your configuration accordingly?\n\nIf your issue is related to `Jackson2ExecutionContextStringSerializer` in 5.2.x, please provide a minimal example or a failing test and I will plan the fix in the next patch release of 5.2.x."
      },
      {
        "author": "andrianov17",
        "created_at": "2026-01-13T03:24:44Z",
        "body": "Jackson2ExecutionContextStringSerializer is working fine in 5.x, but was not adjusted accordingly in 6.x and serialization (and deserialization) was broken.\n\nIt is just deprecated and recommended to migrate to Jackson 3, so until removed it should be functional for those how cannot migrate to Jackson 3 right away and need some time.\n\nOnce JacksonExecutionContextStringSerializer (Jackson 3) has been fixed, personally we don't need Jackson2ExecutionContextStringSerializer anymore, but others could still need for the reason mentioned above.\n\nSo, it's up to you how to proceed with Jackson2ExecutionContextStringSerializer. If you don't want to fix it now - wait for another similar issue.\n\nPlease however merge the PR because otherwise JacksonExecutionContextStringSerializer serializes some noise with doesn't make sense. BTW, the same PR offers fix for Jackson2ExecutionContextStringSerializer in 6.x"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-13T08:20:26Z",
        "body": "> Jackson2ExecutionContextStringSerializer is working fine in 5.x, but was not adjusted accordingly in 6.x and serialization (and deserialization) was broken.\n\nWhich deserialisation was broken? Are you trying to deserialise something with v6 that was serialised with v5? All jobs that were started with v5 should be run to completion (success or failure) with v5 (if a job failed with v5, it should be restarted with v5, not v6). Please make sure to run all your jobs to completion with v5 before upgrading to v6.\n\nNow if you start a job with v6, the deserialisation will work fine. The serialisation/deserialization is not compatible between Spring Batch 5 / Jackson 2 and Spring Batch 6 / Jackson 3 (due to Jackson).\n\n> It is just deprecated and recommended to migrate to Jackson 3, so until removed it should be functional for those how cannot migrate to Jackson 3 right away and need some time.\n\nIt is functional if you use it to deserialise a context that was serialised with it, as explained in the previous point\n\n> So, it's up to you how to proceed with Jackson2ExecutionContextStringSerializer. If you don't want to fix it now - wait for another similar issue.\n\nIt's not that I don't want (I hope my previous message did not imply that), it's about in which branch/version to fix it, as explained in https://github.com/spring-projects/spring-batch/pull/5193#issuecomment-3740260690.\n\n> Please however merge the PR because otherwise JacksonExecutionContextStringSerializer serializes some noise with doesn't make sense. BTW, the same PR offers fix for Jackson2ExecutionContextStringSerializer in 6.x\n\nYes that PR LGTM, but the fix of `Jackson2ExecutionContextStringSerializer` should go in `5.2.x`, not in `main`. We do not put additional effort to maintain deprecated APIs in v6, but we can fix them in v5 if needed."
      },
      {
        "author": "andrianov17",
        "created_at": "2026-01-13T13:05:43Z",
        "body": "Let's get back to the original issue.\n\nEnvironment has been upgraded from Spring 5 to Spring 6. Existing Jackson2ExecutionContextStringSerializer was used (Jackson 3 migration to be done later). That's it.\n\nNow, when you attempt to run a job having JobStep (nested job in its definition), it fails right away on SERIALIZATION - see stack trace above."
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-14T10:05:24Z",
        "body": "Thank you for the feedback. I don't know how I missed the detail that the issue is about `JobStep`, my bad, apologies. That's why it's always better to provide a failing example (we provide a comprehensive [issue reporting guide](https://github.com/spring-projects/spring-batch/blob/main/ISSUE_REPORTING.md) with a project template and everything needed to make reporting issues as easy as possible and save everyone's time).\n\nThe following sample fails as reported:\n\n```java\npackage org.springframework.batch.samples.helloworld;\n\nimport org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;\nimport org.springframework.batch.core.configuration.annotation.EnableJdbcJobRepository;\nimport org.springframework.batch.core.job.Job;\nimport org.springframework.batch.core.job.builder.JobBuilder;\nimport org.springframework.batch.core.repository.ExecutionContextSerializer;\nimport org.springframework.batch.core.repository.JobRepository;\nimport org.springframework.batch.core.repository.dao.Jackson2ExecutionContextStringSerializer;\nimport org.springframework.batch.core.step.Step;\nimport org.springframework.batch.core.step.builder.StepBuilder;\nimport org.springframework.batch.infrastructure.repeat.RepeatStatus;\nimport org.springframework.batch.samples.common.DataSourceConfiguration;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Import;\n\n@Configuration\n@EnableBatchProcessing\n@EnableJdbcJobRepository\n@Import(DataSourceConfiguration.class)\npublic class HelloWorldJobConfiguration {\n\n\t@Bean\n\tpublic Step outerStep(JobRepository jobRepository) {\n\t\tStep innerStep = new StepBuilder(\"inner-step\", jobRepository).tasklet((contribution, chunkContext) -> {\n\t\t\tSystem.out.println(\"Hello from inner step!\");\n\t\t\treturn RepeatStatus.FINISHED;\n\t\t}).build();\n\t\tJob innerJob = new JobBuilder(\"inner-job\", jobRepository).start(innerStep).build();\n\t\treturn new StepBuilder(\"outer-step\", jobRepository).job(innerJob).build();\n\t}\n\n\t@Bean\n\tpublic Job outerJob(JobRepository jobRepository, Step outerStep) {\n\t\treturn new JobBuilder(\"outer-job\", jobRepository).start(outerStep).build();\n\t}\n\n\t@Bean\n\tpublic ExecutionContextSerializer executionContextSerializer() {\n\t\treturn new Jackson2ExecutionContextStringSerializer();\n\t}\n\n}\n```\n\nNow that I fully understand the issue, `Jackson2ExecutionContextStringSerializer` should be adapted in v6 as well. I will merge #5193 for that."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": [
        "0116494b54a92bde25966071a56adf50ec198d64",
        "2a5646a2dee92e4556c71c39719e3cfed34d0a74",
        "72c4aa2779184528aca9b97b4c8f4a6fa3473add",
        "0bb92d54504dfcc2dcb17989f5120f29a9a23261",
        "79f679f9ed91f399c67f3f56b07d8a61c742ab47"
      ]
    }
  },
  {
    "number": 5199,
    "title": "ChunkOrientedStep#doExecute updates the StepExecution outside of the chunk transaction boundary.",
    "state": "open",
    "created_at": "2026-01-06T13:02:24Z",
    "updated_at": "2026-01-12T19:01:11Z",
    "author": "KILL9-NO-MERCY",
    "url": "https://github.com/spring-projects/spring-batch/issues/5199",
    "body": "Hi Spring Batch team ðŸ‘‹\nFirst of all, thank you for your continued work on Spring Batch.\n\n## Description\nThis issue is related to a past change in PR #5165: https://github.com/spring-projects/spring-batch/pull/5165\n\nThis was my mistake, and I wanted to report it properly after noticing an unintended side effect in Spring Batch 6.0.1.\n\nIn Spring Batch 6, ChunkOrientedStep#doExecute updates the StepExecution outside of the chunk transaction boundary.\nBecause of this, if JobRepository.update(stepExecution) fails, the chunk transaction has already been completed, which can leave batch metadata in an inconsistent state.\n\nIn other words, chunk processing and step execution persistence are no longer atomic in ChunkOrientedStep.\n\n## Environment\nspring batch 6.0.1\nChunkOrientedStep#doExecute()\n\n## expected behavior\nThe update of StepExecution via JobRepository.update(stepExecution) should occur within the same transaction boundary as chunk processing.\n\nIf the metadata update fails, the chunk transaction should be rolled back accordingly, preserving consistency between:\nprocessed data and batch metadata.\n\nThis is the behavior historically provided by TaskletStep, where JobRepository.update() is invoked before transaction commit, not after.\n\n## Additional context\nThe current implementation of ChunkOrientedStep#doExecute looks like this (simplified):\n```java\nthis.transactionTemplate.executeWithoutResult(transactionStatus -> {\n    processNextChunk(transactionStatus, contribution, stepExecution);\n});\n\n// transaction already completed here\ngetJobRepository().update(stepExecution);\n\n```\n\n## Proposed fix\n```java\nthis.transactionTemplate.executeWithoutResult(transactionStatus -> {\n    processNextChunk(transactionStatus, contribution, stepExecution);\n    getJobRepository().update(stepExecution);\n});\n```\n\nso that chunk processing and metadata updates share the same transactional boundary.\n\nplease let me know if youâ€™d like me to provide a reproducer or a failing test for this issue. ðŸ™\n",
    "labels": [
      "type: bug",
      "in: core"
    ],
    "comments": [],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5207,
    "title": "Fix typo in whatsnew.adoc and in integration tests",
    "state": "closed",
    "created_at": "2026-01-10T08:11:09Z",
    "updated_at": "2026-01-12T18:44:53Z",
    "author": "wocks1123",
    "url": "https://github.com/spring-projects/spring-batch/issues/5207",
    "body": "fixed typo in the test method and an example code in the 'whatsnew.adoc' document\n\n- wrong : faultToleranChunkOrientedStep, nonRetrybaleExceptions\n- correct : faultToleran**t**ChunkOrientedStep, nonRetry**ab**leExceptions\n",
    "labels": [
      "in: documentation",
      "type: bug"
    ],
    "comments": [
      {
        "author": "wocks1123",
        "created_at": "2026-01-10T08:14:42Z",
        "body": "fix this issue in this PR #5206"
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-12T18:44:52Z",
        "body": "Resolved with #5206 . Thank you for opening the issue and for providing a fix ðŸ™"
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  },
  {
    "number": 5212,
    "title": "Update project template in issue reporting guide",
    "state": "open",
    "created_at": "2026-01-14T10:17:08Z",
    "updated_at": "2026-01-14T16:14:49Z",
    "author": "fmbenhassine",
    "url": "https://github.com/spring-projects/spring-batch/issues/5212",
    "body": "The project template in the [issue reporting guide](https://github.com/spring-projects/spring-batch/blob/main/ISSUE_REPORTING.md) is still using Spring Batch 5. This issue is to update the dependencies and upload a new zip file.\n\nPS: We need to think about a better way to provide such a starter without having to update/upload a zip file every time (maybe a project template under source control to clone by issue reporters)",
    "labels": [
      "type: task",
      "status: for-internal-team"
    ],
    "comments": [
      {
        "author": "scordio",
        "created_at": "2026-01-14T15:03:08Z",
        "body": "> PS: We need to think about a better way to provide such a starter without having to update/upload a zip file every time (maybe a project template under source control to clone by issue reporters)\n\nNot sure if it helps but GitHub has the concept of [template repositories](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template)."
      },
      {
        "author": "fmbenhassine",
        "created_at": "2026-01-14T15:43:29Z",
        "body": "Thank you for the suggestion! We do [not](https://github.com/spring-projects?q=&type=template&language=&sort=) use template repositories in our org as we might end up with many of them which will become unmanageable for us."
      }
    ],
    "closing_references": {
      "pull_requests": [],
      "commits": []
    }
  }
]